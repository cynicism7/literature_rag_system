{"chunk_id": 0, "text": "Citation: Zhang, F.; Chen, Z. A Novel Reinforcement Learning-Based Particle Swarm Optimization Algorithm for Better Symmetry between Convergence Speed and Diversity. Symmetry 2024, 16, 1290. https://doi.org/10.3390/sym16101290 Academic Editors: Tomohiro Inagaki and Sergei Odintsov Received: 31 July 2024 Revised: 11 September 2024 Accepted: 25 September 2024 Published: 1 October 2024 Copyright: \u00a9 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). symmetry S S Article A Novel Reinforcement Learning-Based Particle Swarm Optimization Algorithm for Better Symmetry between Convergence Speed and Diversity Fan Zhang 1 and Zhongsheng Chen 2,* 1 The Sixty-Third Research Institute, National University of Defense Technology, Nanjing 210001, China; 120810119_zf@alu.hit.edu.cn 2 College of Automotive Engineering, Changzhou Institute of Technology, Changzhou 213032, China * Correspondence: czhongsheng0803@163.com Abstract: This paper introduces a novel Particle Swarm Optimization (RLPSO) algorithm based on reinforcement learning, embodying a fundamental symmetry between global and local search processes. This symmetry aims at addressing the trade-off issue between convergence speed and diversity in traditional algorithms. Traditional Particle Swarm Optimization (PSO) algorithms often struggle to maintain good convergence speed and particle diversity when solving multi-modal function problems. To tackle this challenge, we propose a new algorithm that incorporates the principles of reinforcement learning, enabling particles to intelligently learn and adjust their behavior for better convergence speed and richer exploration of the search space. This algorithm guides particle learning behavior through online updating of a Q-table, allowing particles to selectively learn effective information from other particles and dynamically adjust their strategies during the learning process, thus finding a better balance between convergence speed and diversity. The results demonstrate the superior performance of this algorithm on 16 benchmark functions of the CEC2005 test suite compared to three other", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 1, "text": "particles to selectively learn effective information from other particles and dynamically adjust their strategies during the learning process, thus finding a better balance between convergence speed and diversity. The results demonstrate the superior performance of this algorithm on 16 benchmark functions of the CEC2005 test suite compared to three other algorithms. The RLPSO algorithm can find all global optimum solutions within a certain error range on all 16 benchmark functions, exhibiting outstanding performance and better robustness. Additionally, the algorithm\u2019s performance was tested on 13 benchmark functions from CEC2017, where it outperformed six other algorithms by achieving the minimum value on 11 benchmark functions. Overall, the RLPSO algorithm shows significant improvements and advantages over traditional PSO algorithms in aspects such as local search strategy, parameter adaptive adjustment, convergence speed, and multi-modal problem handling, resulting in better performance and robustness in solving optimization problems. This study provides new insights and methods for the further development of Particle Swarm Optimization algorithms. Keywords: PSO algorithm; convergence; diversity; reinforcement learning; Q-learning; Q-table 1. Introduction The PSO algorithm is a population-based stochastic optimization technique developed by Kennedy and Eberhart in 1995 [1]. The algorithm is inspired by the behavior of birds foraging, and finds the optimal solution by simulating the process of particles moving in the search space. The PSO algorithm has the advantages of a simple implementation, high computational efficiency, and easy convergence, and has quickly become an effective tool to solve complex optimization problems. The core idea of the PSO algorithm is to find the optimal solution by simulating the process of particles flying in the search space. In the algorithm, each particle represents a potential solution, and the position of the particle is updated by adjusting its position and velocity. The particle adjusts its position according to its own experience and the experience", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 2, "text": "the optimal solution by simulating the process of particles flying in the search space. In the algorithm, each particle represents a potential solution, and the position of the particle is updated by adjusting its position and velocity. The particle adjusts its position according to its own experience and the experience of the group to find the global optimal solution. This way of simulating the behavior of swarm intelligence in nature makes the Symmetry 2024, 16, 1290. https://doi.org/10.3390/sym16101290 https://www.mdpi.com/journal/symmetry Symmetry 2024, 16, 1290 2 of 27 PSO algorithm perform well in dealing with high-dimensional, nonlinear and multi-modal optimization problems. With its fast computation speed and relatively good stability, the PSO algorithm has been widely applied in various fields, such as neural network training [2,3], fault diagnosis [4,5], economics and pattern recognition, power system optimization, signal processing, data mining, image processing, finance, and medicine. It can be applied to multiple aspects of power systems, including power generation scheduling, grid planning, and power market analysis. It can help optimize the efficiency, reliability, and economy of power systems. Although the traditional PSO algorithm performs well in many problems, it also has some limitations and disadvantages. The traditional PSO algorithm tends to fall into local optima when dealing with complex optimization problems, especially for high- dimensional, non-convex, nonlinear problems, where particles may prematurely converge to local optima and fail to discover the global optimum. The convergence speed of the PSO algorithm may be slow, and especially when the search space of the optimization problem is large or the number of particles is small, it may take a long time to converge. The traditional PSO algorithm involves many parameters, such as inertia weight and acceleration coefficient, whose values have a significant impact on the performance of the algorithm. Lacking a universal selection method, experimentation and adjustment", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 3, "text": "large or the number of particles is small, it may take a long time to converge. The traditional PSO algorithm involves many parameters, such as inertia weight and acceleration coefficient, whose values have a significant impact on the performance of the algorithm. Lacking a universal selection method, experimentation and adjustment are required to determine the optimal parameter values. Additionally, the PSO algorithm is sensitive to the initial conditions and parameter settings of the problem, which makes the algorithm unstable in some cases and susceptible to noise and interference. As regards premature convergence and premature stagnation, in some cases, the PSO algorithm may miss better solutions due to premature convergence, or experience premature stagnation, leading to algorithmic stagnation. Currently, various improved and optimized PSO algorithms have been proposed by researchers to address the shortcomings of traditional PSO algorithms, such as with param- eter adjustments [6\u20139], multi-strategy cooperative PSOs [10\u201317], and hybrid evolutionary algorithms [18\u201324]. Based on existing research, improvements in PSO algorithms can be mainly classified into the following three categories: Class one: Adaptive parameter adjustment. This category of algorithms mainly adjusts the design parameters of the PSO algorithm, such as the inertia weight of velocity and the weights between individual best (pbest) and global best (gbest), to change the convergence speed of the algorithm. For example, the PSO with inertia weight (PSO-w) algorithm is used to increase the convergence speed [6], while Ratnaweera et al. proposed various inertia weight adjustment strategies [7]. Additionally, some algorithms such as the Q- learning-based Particle Swarm Optimization (QLPSO) [8] and Adaptive Weighted PSO (AWPSO) [9] algorithms adjust the convergence speed by controlling algorithm parameters. Class two: Comprehensive learning from other particles. The main idea of such algorithms is to fully utilize information from other particles to update the flight speed and position of the", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 4, "text": "Particle Swarm Optimization (QLPSO) [8] and Adaptive Weighted PSO (AWPSO) [9] algorithms adjust the convergence speed by controlling algorithm parameters. Class two: Comprehensive learning from other particles. The main idea of such algorithms is to fully utilize information from other particles to update the flight speed and position of the current particle, including the current positions and pbests of other particles. Every particle in the swarm can comprehensively learn from other particles. In the Fully Informed Particle Swarm (FIPS) algorithm, each particle utilizes information from all neighboring particles, not just from gbest [10]. In the PSO-w-local algorithm, each particle compares the performances of every other member in the social network structure and imitates the best-performing particle [11]. The Cooperative Particle Swarm Optimization (CPSO) algorithm divides the particle swarm into multiple subgroups to optimize different components of the solution vector cooperatively. In each iteration, the CPSO algorithm uses each dimension of particles to update gbest, avoiding the issue of \u201ctwo steps forward, one step back\u201d [12]. The Comprehensive Learning Particle Swarm Optimization (CLPSO) algorithm utilizes pbests of all other particles to update the velocity of particles. This learning strategy can prevent premature convergence [13]. The Example- based Learning Particle Swarm Optimization (ELPSO) algorithm proposes a strategy to update particle positions using an example set of multiple global best particles [14]. The Heterogeneous Comprehensive Learning Particle Swarm Optimization (HCLPSO) Symmetry 2024, 16, 1290 3 of 27 algorithm divides the swarm into two subgroups, one focusing on exploration and the other on exploitation [15]. The Terminal Crossover and Direction-based Particle Swarm Optimization (TCSPSO) utilizes pbest to enhance population diversity at the terminal stage of iteration, helping particles jump out of local optima [16]. G.P. Xu et al. proposed the Two- Swarm Learning PSO (TSLPSO) algorithm, which is a dimensional learning strategy (DLS) for", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 5, "text": "[15]. The Terminal Crossover and Direction-based Particle Swarm Optimization (TCSPSO) utilizes pbest to enhance population diversity at the terminal stage of iteration, helping particles jump out of local optima [16]. G.P. Xu et al. proposed the Two- Swarm Learning PSO (TSLPSO) algorithm, which is a dimensional learning strategy (DLS) for discovering and integrating promising information of the population best solution [17]. Class three: Hybrid particle swarm optimization. This category of algorithms inte- grates different optimization ideas to improve traditional PSO algorithms. For example, the PSO-GA algorithm incorporates mutation operators from genetic algorithms into the PSO algorithm [18,19]. Uriarte A et al. integrated the gradient descent method (BP algo- rithm) as an operator into the PSO algorithm [20]. Additionally, hybrid particle swarm optimization algorithms combine the PSO algorithm with other optimization techniques such as simulated annealing [21]. Aydilek et al. proposed a hybrid (HFPSO) algorithm that combines advantages of the firefly algorithm and PSO algorithm [22]. Moreover, the PSO-CL algorithm adopts a crossover learning strategy, utilizing a comprehensive learning strategy (CCL) and stochastic example learning strategy (SEL), to balance global exploration and local exploitation capabilities [23]. Zhang et al. constructed the TLS-PSO algorithm, utilizing a worst\u2013best example learning strategy, to achieve a hybrid learning mechanism with three learning strategies in PSO [24]. Despite the improved performance of PSO algorithms in respective problems, they still have limitations when solving complex problems. For example, a high convergence speed can quickly approach individual best points (pbest) and global best points (gbest), but it may lead to the loss of diversity in the particle swarm, especially when gbest and pbest are far from the global optimum but close to each other [14]. The good diversity of the particle swarm ensures the algorithm\u2019s global search capability but may result in slow convergence. According to the \u201cno", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 6, "text": "lead to the loss of diversity in the particle swarm, especially when gbest and pbest are far from the global optimum but close to each other [14]. The good diversity of the particle swarm ensures the algorithm\u2019s global search capability but may result in slow convergence. According to the \u201cno free lunch\u201d theorem [25], many improved PSO algorithms may still fall into local optima or converge too slowly when solving complex problems. Especially when balancing convergence speed and diversity, existing algorithms often fail to achieve ideal results. The RLPSO algorithm proposed in this paper is inspired by the comprehensive learn- ing strategy of CLPSO and integrates reinforcement learning policies. Through intelligent learning strategies, it achieves a better balance between convergence speed and diversity, endowing particles with more intelligent learning strategies, effective global search capabil- ities, dynamic adjustment of learning strategies, and a good balance between convergence speed and diversity, as well as wide applicability. This enables more effective information sharing and utilization during the learning process, providing an efficient and intelligent optimization method for problem-solving. RLPSO has a wide range of applications and can play an important role in engineering [4], science [11], finance [19], medicine [26], and other fields by improving the performance and robustness of optimization algorithms, thus providing effective solutions to practical problems. The remaining parts of this paper are as follows. In Section 2, the theoretical foundation of the RLPSO algorithm is briefly introduced. In Section 3, the execution process of the RLPSO algorithm is explained in detail. In Section 4, we discuss parameter selection and the role of Q-learning, while also selecting 29 benchmark functions to validate the RLPSO algorithm. Finally, conclusions with a discussion and summary are given in Section 5. 2. The Basic Principle of the CLPSO Algorithm In the PSO algorithm, the", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 7, "text": "in detail. In Section 4, we discuss parameter selection and the role of Q-learning, while also selecting 29 benchmark functions to validate the RLPSO algorithm. Finally, conclusions with a discussion and summary are given in Section 5. 2. The Basic Principle of the CLPSO Algorithm In the PSO algorithm, the velocity Vd i update and position Xd i update of the dth dimen- sion of the particle i are represented by Equation (1) and Equation (2), respectively [19]. Vd i = Vd i + c1 \u00d7 rd 1 \u00d7 (pbestd i \u2212Xd i ) + c2 \u00d7 rd 2 \u00d7 (gbestd \u2212Xd i ) (1) Xd i = Xd i + Vd i \u2206t (2) Symmetry 2024, 16, 1290 4 of 27 Xi = (X1 i , X2 i , . . . , XD i ) and Vi = (V1 i , V2 i , . . . , VD i ) represent the position and velocity of the particle i, D is the number of dimensions, pbesti = (pbest1 i , pbest2 i , . . . , pbestD i ) is the best-so-far position of the particle i, gbest = (gbest1, gbest2, . . . , gbestD) is the best-so-far position of the whole swarm, c1 and c2 are constant weights for pbest and gbest, respectively, rd 1 and rd 2 are two random numbers in the range [0,1]; \u2206t = 1. If Vd i > Vd max, then Vd i = Vd maxsign(Vd i ), where Vd max is maximum allowable velocity of the dth dimension. The CLPSO algorithm has a solid theoretical foundation, wide application, and effec- tively utilizes swarm intelligence to address multimodal problems, demonstrating a certain degree of reliability and effectiveness, especially in tackling multimodal optimization prob- lems. The CLPSO algorithm [13] can", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 8, "text": "Vd max is maximum allowable velocity of the dth dimension. The CLPSO algorithm has a solid theoretical foundation, wide application, and effec- tively utilizes swarm intelligence to address multimodal problems, demonstrating a certain degree of reliability and effectiveness, especially in tackling multimodal optimization prob- lems. The CLPSO algorithm [13] can maintain a good diversity within the swarm; it excels particularly in tackling multimodal problems. Therefore, as the theoretical foundation of the RLPSO algorithm, CLPSO possesses a certain degree of reliability and effectiveness in addressing multimodal optimization problems. At the core of the CLPSO algorithm lies the updating rule, which serves as its main concept. The specific formulas are given as Equations (3) and (4). Vd i = wkVd i +c \u00d7 r \u00d7 (pbestd fi(d) \u2212Xd i ) (3) Xd i = Xd i + Vd i \u2206t (4) \u03c9k is inertia weight when the number of iterations is k; fi = [ fi(1), fi(2), . . ., fi(D)] defines the particles\u2019 pbests that the particle i should follow.pbestd fi(d) can be the corresponding dimension of any particle\u2019s pbest as the selection of fi(d) depends on the probability Pci. The probability of selecting the i particle\u2019s pbest is (1 \u2212Pci), while the probability of selecting other particles\u2019 pbest is Pci. The value of Pci is computed as in Equation (5). Pci = 0.05 + 0.45 \u00d7 \u0014 exp \u001210(i \u22121) ps \u22121 \u0013 \u22121 \u0015 /[exp(10) \u22121] (5) where ps is the population size of the swarm, and i is the particle\u2019s id counter. In this paper, the objective of the PSO algorithm and its various variants is to locate the global minimum [3,4]. Figure 1 illustrates the flowchart of the CLPSO algorithm. Figure 1 illustrates how the CLPSO algorithm updates pbest and gbest by learning from other particles, although", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 9, "text": "the particle\u2019s id counter. In this paper, the objective of the PSO algorithm and its various variants is to locate the global minimum [3,4]. Figure 1 illustrates the flowchart of the CLPSO algorithm. Figure 1 illustrates how the CLPSO algorithm updates pbest and gbest by learning from other particles, although the particles being learned are randomly selected. This may lead to certain particles being unable to learn from superior ones and deriving no valuable insights from inferior ones. Consequently, learning may occasionally prove ineffective, causing the CLPSO algorithm to converge slowly and underutilize swarm information. For instance, considering the equation f (X) = d \u2211 i=1 \b Xi 2, we assume the following conditions: V1 = (0, 0, 0), w =1, c =2, r =0.5, X1 = (6, 2, 0), pbest1 = (4, 1, 4), pbest2 = (1, 2, 5), pbest3 = (1, 2, 1) The first, second, and third dimensions of this particle learn from pbest1, pbest2, and pbest3, respectively. X1 is updated according to Equations (3) and (4); we can obtain a new X1_new = (4, 2, 1) and f (X1_new) = 21, which is better than f (X1) = 40 and f (pbest1) = 33. So, pbest1 = (4, 2, 1) is updated to pbest1 = X1_new = (4, 2, 1). For this case, although the new fitness value is better, the second dimension of X1_new is not updated (still at 2), and the third dimension is updated to be farther away from the optimal point (0, 0, 0), shifting from 0 to 1. Each dimension of a particle has the potential to learn from a different particle, but the particles learned may not necessarily be optimal within the context of the CLPSO algorithm. Therefore, we adjust the particle learning strategy based on the CLPSO algorithm, optimizing", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 10, "text": "0, 0), shifting from 0 to 1. Each dimension of a particle has the potential to learn from a different particle, but the particles learned may not necessarily be optimal within the context of the CLPSO algorithm. Therefore, we adjust the particle learning strategy based on the CLPSO algorithm, optimizing the learning objects and enhancing learning efficiency. Symmetry 2024, 16, 1290 5 of 27 Figure 1. A flowchart of the CLPSO algorithm. Note: the meanings of the parameters in the figure are shown in Table 1. Table 1. The meanings of the parameters for the CLPSO algorithm. Parameters The Meaning of the Parameters \u03c90 inertia weight of the first iteration \u03c91 inertia weight of the last iteration PSO the PSO algorithm ps population size max_gen maximum generations k generation counter i particle\u2019s id counter d dimension \u03c9(k) inertia weight gbestd the dth dimension\u2019s value of gbest flagi the number of generations for which the particle i has not improved its pbest, and the initial value for flagi of the particle i is 0. Symmetry 2024, 16, 1290 6 of 27 3. Reinforcement Learning-Based PSO Algorithm To address the limitations of the aforementioned PSO algorithms, we introduce a novel PSO algorithm incorporating reinforcement-learning principles. In the RLPSO algorithm, particles consistently learn from superior peers while preserving diversity. Rather than ran- domly selecting particles from the swarm, each particle chooses its learned peers based on the Q table. Moreover, to maximize the effectiveness of each learning instance, the RLPSO algorithm updates every dimension of the global best (gbest) using all recently updated personal bests (pbests). Further insights into the RLPSO algorithm are provided below. 3.1. Q-Learning in RLPSO Reinforcement learning (RL) is a branch of machine learning that dictates how ma- chines should behave in their current environment to maximize cumulative rewards.", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 11, "text": "dimension of the global best (gbest) using all recently updated personal bests (pbests). Further insights into the RLPSO algorithm are provided below. 3.1. Q-Learning in RLPSO Reinforcement learning (RL) is a branch of machine learning that dictates how ma- chines should behave in their current environment to maximize cumulative rewards. Within machine learning, there are three fundamental components: state, action, and reward [26]. Q-learning, first introduced by Watkins in 1989 [27], is a specific type of RL algorithm. Q-learning involves the creation and updating of a Q table, which guides the machine\u2019s actions based on the current state. In some scenarios, the machine selects the action with the highest Q value from the Q table, updating the Q table during training. The updated policy is illustrated in Equation (6). Q(s, a) = Q(s, a) + \u03b1\u00d7[R(s, a) + \u03b3\u00d7maxa\u2032Q(s\u2032, a\u2032) \u2212Q(s, a)] (6) \u03b1 is the learning rate, \u03b3 is the discount factor, R(s, a) is the immediate reward acquired from executing action a under the state s, Q(s, a) is an accumulative reward, and s\u2032 is the next state of the state s when executing action a under the state s; a\u2032 is an optional action under the state s\u2032 and max a\u2032 (s\u2032, a\u2032) is the maximum Q value that can be obtained at the state s\u2032. The model of Q-learning is shown in Figure 2. Figure 2. The model of Q-learning. In the RLPSO algorithm, we randomly generate a (D \u00d7 ps)-size Q table for each particle of the swarm. D is the number of particle dimensions; ps is the population size of the swarm. The Q table dictates from which particle\u2019s pbest each dimension of every particle learns during each iteration. When updating each dimension of the particle, the particle with the highest Q value becomes", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 12, "text": "of the swarm. D is the number of particle dimensions; ps is the population size of the swarm. The Q table dictates from which particle\u2019s pbest each dimension of every particle learns during each iteration. When updating each dimension of the particle, the particle with the highest Q value becomes the focal point of learning. In instances where the particle with the highest Q value is itself, the particle can still learn from its own past experiences. Each particle maintains its own Q table, as illustrated in Figure 3. Symmetry 2024, 16, 1290 7 of 27 Figure 3. The Q table of the particle i. As shown in Figure 3, each dimension of the particle i has only one state, which is how each dimension learns from other particles\u2019 pbests. Every dimension of the particle i has ps actions, that is, selects which particle\u2019s pbest to learn from ps particles. Thus, Equation (6) can be simplified to Equation (7): Qd,fi(d) = Qd,fi(d)+\u03b1 \u00d7 [R + \u03b3 \u00d7 maxQd \u2212Qd,fi(d)] (7) d represents the dth dimension of the particle i, fi(d) represents which particle\u2019s pbest the dth dimension of the particle i learns from, Qd,fi(d) is the Q value that the dth dimension of the particle i can obtain when learning from the pbest of particle fi(d), and maxQd is the largest Q value of the dth dimension in the Q table of the particle i. To effectively harness the collective knowledge within the particle swarm, we employ Q-learning for selecting learned particles, rather than resorting to random selection from the swarm. Within the RLPSO algorithm, particles initially choose particles to learn from randomly with a certain probability, thereby exploring the solution space extensively and updating the Q table at the onset of iterations. As the iterations progress, particles gradually adjust", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 13, "text": "learned particles, rather than resorting to random selection from the swarm. Within the RLPSO algorithm, particles initially choose particles to learn from randomly with a certain probability, thereby exploring the solution space extensively and updating the Q table at the onset of iterations. As the iterations progress, particles gradually adjust their selection of particles to learn from based on the Q table, thereby expediting convergence and preventing divergence in certain dimensions. Depending on the outcomes of the updates, different rewards are assigned during the Q table update process. The update strategy is outlined as follows: when gbest undergoes an update, it receives the highest \u201cglobal reward\u201d as an immediate reward for updating the Q table. Conversely, when pbest is updated, it receives a larger \u201clocal reward\u201d as an immediate incentive. In cases where no update occurs, a \u201cpenalty\u201d is assigned. The Q table is updated according to Equation (7). 3.2. The Strategy of Selecting Learned Particles During each particle\u2019s updating process, the RLPSO algorithm randomly generates a number dimup from 1 to D as the number of dimensions which need updating, and randomly selects dimup dimensions from the particle to be updated, leaving the unselected dimensions unchanged. In the learning process of each dimension within the particle, the algorithm randomly selects a particle from the swarm of learning with a certain probability \u03b5k and selects a particle to learn from according to the Q table with probability (1 \u2212\u03b5k). Then, Vd i and Xd i are updated according to Equation (3) and Equation (4), respectively. After each iteration, \u03b5k updates according to Equation (8); des is the magnitude of the descent per iteration, and k is the number of iterations. \u03b5k+1 = \u03b5k \u00d7 (1\u2212des) (8) Symmetry 2024, 16, 1290 8 of 27 Equation (8) reveals that as iterations progress,", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 14, "text": "(3) and Equation (4), respectively. After each iteration, \u03b5k updates according to Equation (8); des is the magnitude of the descent per iteration, and k is the number of iterations. \u03b5k+1 = \u03b5k \u00d7 (1\u2212des) (8) Symmetry 2024, 16, 1290 8 of 27 Equation (8) reveals that as iterations progress, the likelihood of selecting learned parti- cles based on the Q table gradually escalates. Consequently, particles begin to incrementally glean insights from superior counterparts, as dictated by the Q table. In contrast to the CLPSO algorithm, which relies on the fitness values of two randomly selected particles to determine learned particles, the RLPSO algorithm expedites this process by leveraging the Q table. The selection process of fi(d) for the particle i is shown in Figure 4. Simultaneously, we update the Q table of particle i in response to rewards or penalties incurred during the updating of gbest and pbest. If the dimension that requires updating is denoted by d, we update Qd,fi(d) in accordance with Equation (7). Otherwise, no updates are made. We repeat this process from d = 1 until d = D, resulting in the complete update of particle i\u2019s Q table. Figure 4. The flowchart of selecting learned particles for the particle i. 3.3. The Full Flow of the RLPSO Algorithm In this section, we present the specific process of RLPSO, outlined as follows: Step 1: We initialize the parameters of the RLPSO algorithm, including the position X, associated velocities V, Q table, \u03b50, pbest, and gbest of population; set k = 0. Step 2: Each dimension of the particle i selects the learned particle fi(d) according to Figure 4 to obtain a new velocity Vi and position Xi. Step 3: For the particle i, if the pbest is updated, we update each dimension of gbest based", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 15, "text": "of population; set k = 0. Step 2: Each dimension of the particle i selects the learned particle fi(d) according to Figure 4 to obtain a new velocity Vi and position Xi. Step 3: For the particle i, if the pbest is updated, we update each dimension of gbest based on the 1 to D dimensions of the pbest. This ensures that certain dimensions of gbest do not stray too far from the optimal point. We can view each dimension of pbest as a gene, with gbest selectively inheriting useful genes from pbest. If gbest is not updated, we set flagi = flagi + 1. If flagi \u2265m, run the PSO algorithm and reset flagi = 0. Different from the CLPSO algorithm where flagi is reset to 0 when pbest is updated, the RLPSO algorithm only resets flagi to 0 when gbest is updated. This encourages particles to choose learned particles based on Q-learning more frequently. Symmetry 2024, 16, 1290 9 of 27 Step 4: If gbest is updated, the particle i receives a global reward; if gbest is not updated but pbest is, the particle i receives a local reward; if neither gbest nor pbest is updated, particle i incurs a penalty. Subsequently, we update the Q table of particle i based on the rewards or penalties obtained from updating gbest and pbest. Step 5: We set i = i +1; repeat Step 2, Step 3, and Step 4 until i equals the population size ps. Step 6: We set k = k + 1; repeat Step 2, Step 3, Step 4, and Step 5 until d equals the maximum number of iterations. A detailed description of algorithm pseudocode is shown in Algorithm 1, and the entire flowchart of the RLPSO algorithm is illustrated in Figure 5. The parameters", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 16, "text": "set k = k + 1; repeat Step 2, Step 3, Step 4, and Step 5 until d equals the maximum number of iterations. A detailed description of algorithm pseudocode is shown in Algorithm 1, and the entire flowchart of the RLPSO algorithm is illustrated in Figure 5. The parameters shown in Algorithm 1 and Figure 5 are consistent with those set in Figure 1. Algorithm 1: RLPSO algorithm Input: Initialize position X, associated velocities V, Q table, \u03b50, pbest and gbest of population, and set k = 0, flag = 0, m = 10; Output: Optimal solution; 1 for k < max_gen do 2 for I < ps do 3 if flagi \u2265 m then 4 run PSO algorithm and reset flagi = 0; 5 End 6 for d < D do 7 Each dimension of the particle i selects the learned particle fi(d) according 8 to Figure 4 to get new velocity Vi and position Xi; 9 End 10 if Fit (Xi) < Fit (pbest) 11 Update the pbest of particle i; 12 End 13 if the pbest of particle i is updated then 14 we will update each dimension of gbest basis with 1 to D dimensions of 15 the pbest from particle i; 16 else 17 we set flagi = flagi + 1 18 End 19 if gbest is updated, then 20 R = global reward; 21 else if gbest is not updated but pbest is updated, then 22 R = local reward; 23 else if both gbest nor pbest are not updated, then 24 R = get penalty; 25 End 26 Update the Q table of particle i according to reward or penalty getting from 27 updating of gbest and pbest; 28 i = i + 1; 29 End 30 k = k + 1", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 17, "text": "both gbest nor pbest are not updated, then 24 R = get penalty; 25 End 26 Update the Q table of particle i according to reward or penalty getting from 27 updating of gbest and pbest; 28 i = i + 1; 29 End 30 k = k + 1 31 End 32 Output: gbest Symmetry 2024, 16, 1290 10 of 27 Figure 5. A Flowchart of the RLPSO algorithm, cite from Figure 4. Symmetry 2024, 16, 1290 11 of 27 The pseudocode of the RLPSO algorithm reveals a structure consisting of two nested loops. Each iteration within the outer loop corresponds to a particle within the swarm, and within each particle\u2019s iteration, every dimension is iterated over. Thus, the time complexity of the algorithm is influenced by the particle dimension D, the population size ps, and the number of algorithm iterations max_gen. Consequently, the algorithm\u2019s time complexity can be inferred as O(max_gen \u00d7 ps \u00d7 D). 4. Experimental Validations For testing the performance of the RLPSO algorithm, a total of 16 famous benchmark functions were selected from CEC2005 [28], and the performance of the RLPSO algorithm was compared with other PSO algorithms. The 16 benchmark functions include seven unimodal functions and nine multimodal functions to ensure the comprehensiveness of the experiment. The presented algorithm is implemented in Python 3.9 and the program has been run on a i7-8565U @1.80 GHz Intel(R) Core(TM) 4 Duo processor with 8 GB of Random Access Memory (RAM). The tested benchmark functions are listed as follows: Unimodal functions: (1) f 1: sphere model: f1(x) = 30 \u2211 i=1 x2 i , \u2212100 \u2264xi \u2264100, min( f1) = f1(0, . . . , 0) = 0 (2) f 2: Schwefel\u2019s problem 2.22: f2(x) = 30 \u2211 i=1 |xi| + 30 \u220f i=1 |xi|, \u221210 \u2264xi", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 18, "text": "as follows: Unimodal functions: (1) f 1: sphere model: f1(x) = 30 \u2211 i=1 x2 i , \u2212100 \u2264xi \u2264100, min( f1) = f1(0, . . . , 0) = 0 (2) f 2: Schwefel\u2019s problem 2.22: f2(x) = 30 \u2211 i=1 |xi| + 30 \u220f i=1 |xi|, \u221210 \u2264xi \u226410, min( f2) = f2(0, . . . , 0) = 0 (3) f 3: Schwefel\u2019s problem 1.2: f3(x) = 30 \u2211 i=1 i \u2211 j=1 xj !2 , \u2212100 \u2264xi \u2264100, min( f3) = f3(0, . . . , 0) = 0 (4) f 4: Schwefel\u2019s problem 2.21: f4(x) = max{|xi|, 1 \u2264i \u226430}, \u2212100 \u2264xi \u2264100, min( f4) = f4(0, . . . , 0) = 0 (5) f 5: generalized Rosenbrock\u2019s function: f5(x) = 29 \u2211 i=1 [100( xi+1 \u2212x2 i \u00112 +(xi\u22121)2], \u221230 \u2264xi \u226430, min( f5) = f5(1, . . . , 1) = 0 (6) f 6: step function: f6(x) = 30 \u2211 i=1 (\u230axi+0.5\u230b)2, \u2212100 \u2264xi \u2264100, min( f6) = f6(0, . . . , 0) = 0 (7) f 7: quartic function, i.e., noise: f7(x) = 30 \u2211 i=1 ix4 i +random[0, 1), \u22121.28 \u2264xi \u22641.28, min( f7) = f7(0, . . . , 0) = 0 Multimodal functions: (8) f 8: generalized Schwefel\u2019s problem 2.26: f8(x) = 30 \u2211 i=1 \u0012 xisin( q |xi|)), \u2212500 \u2264xi \u2264500, min( f8) = f8(420.9687, . . . , 420.9687) = \u221212569.5 (9) f 9: generalized Rastrigin\u2019s function: Symmetry 2024, 16, 1290 12 of 27 f9(x) = 30 \u2211 i=1 h x2 i \u221210 cos(2\u03c0xi)+10], \u22125.12 \u2264xi \u22645.12, min( f9) = f9(0, . . . , 0) = 0 (10) f 10: Ackley\u2019s function: f10(x) = \u221220exp(\u22120.2 s 1 30 30 \u2211 i=1 x2 i ) \u2212exp( 1 30 30 \u2211 i=1 cos 2\u03c0xi) +", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 19, "text": "f9(x) = 30 \u2211 i=1 h x2 i \u221210 cos(2\u03c0xi)+10], \u22125.12 \u2264xi \u22645.12, min( f9) = f9(0, . . . , 0) = 0 (10) f 10: Ackley\u2019s function: f10(x) = \u221220exp(\u22120.2 s 1 30 30 \u2211 i=1 x2 i ) \u2212exp( 1 30 30 \u2211 i=1 cos 2\u03c0xi) + 20+e, \u221232 \u2264xi \u226432, min( f10) = f10(0, . . . , 0) = 0 (11) f 11: generalized Griewank function: f11(x) = 1 4000 30 \u2211 i=1 x2 i \u2212 30 \u220f i=1 cos( xi \u221a i )+1, \u2212600 \u2264xi \u2264600, min( f11) = f11(0, . . . , 0) = 0 (12) f 12: generalized penalized function: f12(x) = \u03c0 30 \u001a 10 sin2(\u03c0y1)+ 29 \u2211 i=1 {(yi\u22121)2 \u00d7[1 + 10 sin2(\u03c0yi+1)]} + (yn\u22121 \u00012\u001b + 30 \u2211 i=1 ui(xi , 10, 100, 4), yi= 1+1 4(xi+1), u(xi, a, k, m) = \uf8f1 \uf8f2 \uf8f3 k(xi\u2212a)m xi> a 0 \u2212a \u2264xi \u2264a k(\u2212xi\u2212a)m xi< \u2212a \u221250 \u2264xi \u226450, min( f12) = f12(\u22121, . . . , \u22121) = 0 (13) f 13: generalized penalized function: f13(x) = 0.1 \u001a sin2(3\u03c0x1)+ 29 \u2211 i=1 {( xi\u22121)2 \u00d7[1 + 10 sin2(3\u03c0xi+1)]} + (xn\u22121 \u00012 \u00d7 [1+ sin2(2\u03c0x30)] \u001b + 30 \u2211 i=1 ui(xi , 5, 100, 4), \u221250 \u2264xi \u226450, min( f13) = f13(1, . . . , 1) = 0 The function u is the same as above. (14) f 14: six-hump camel-back function: f14(x) = 4x2 1\u22122.1x4 1 + 1 3x6 1 + x1x2\u22124x2 2+4x4 2, \u22125 \u2264xi \u22645 xmin= (0.08983, \u22120.7126), (\u22120.08983, 0.7126), min( f14) = \u22121.0316285 (15) f 15: Branin function: f15(x) = (x2 \u22125.1 4\u03c02 x2 1 + 5 \u03c0x1\u22126)2+10(1\u22121 8\u03c0) cosx1+10, \u22125 \u2264x1 \u226410, 0 \u2264x2 \u226415 xmin= (\u22123.142, 12.275), (3.142, 2.275), (9.425, 2.425), min( f15) = 0.398 (16) f 16: Goldstein\u2013Price function f16(x)", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 20, "text": "\u22645 xmin= (0.08983, \u22120.7126), (\u22120.08983, 0.7126), min( f14) = \u22121.0316285 (15) f 15: Branin function: f15(x) = (x2 \u22125.1 4\u03c02 x2 1 + 5 \u03c0x1\u22126)2+10(1\u22121 8\u03c0) cosx1+10, \u22125 \u2264x1 \u226410, 0 \u2264x2 \u226415 xmin= (\u22123.142, 12.275), (3.142, 2.275), (9.425, 2.425), min( f15) = 0.398 (16) f 16: Goldstein\u2013Price function f16(x) = [1 + ( x1 + x2+1)2 \u00d7 (19 \u221214x1+3x2 1\u221214x2+6x1x2+3x2 2)] \u00d7[30 + (2 x1\u22123x2)2 \u00d7 (18 \u221232x1+12x2 1+48x2\u221236x1x2+27x2 2)] \u22122 \u2264xi \u22642, min( f16) = f16(0, \u22121) = 3 To evaluate the performance of the RLPSO algorithm, comparisons with other algo- rithms were conducted under the same test parameters. The problem dimension, pop- ulation size, maximum number of iterations, and number of independent runs were set uniformly as 30, 40, 5000, and 30 [14]. For the RLPSO algorithm, we randomly generated a Q table of (D \u00d7 ps) size for each particle of the population, and each value in the Q table was randomly generated as an integer between \u221240 and 0. Then, we compared the performance Symmetry 2024, 16, 1290 13 of 27 of the PSO, CLPSO, ELPSO, and RLPSO algorithms using experimental data from an H.D. Shao article [14]. Table 2 presents the parameter settings for all comparison algorithms, as obtained from their respective studies. Table 2. Parameter settings for PSO algorithms. Algorithm Parameter Settings PSO [1] \u03c9 = 0.729, c1 = c2 = 1.494 CLPSO [13] \u03c9 = 0.9\u20130.4, c = 1.494, m = 7 ELPSO [14] \u03c9 = 0.729, c1 = 1.49445, c2 = 1.494, m = 7, Bm = 4 RLPSO \u03c9 = 0.9\u20130.4, c = 1.49445, m = 10, dimup = 30, global reward = 10, local reward = 2, penalty = \u22121, \u03b1 = 0.1, \u03b3 = 0.95, \u03b50 = 0.6, des = 0.001 4.1. An Analysis of the Role of", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 21, "text": "1.494, m = 7, Bm = 4 RLPSO \u03c9 = 0.9\u20130.4, c = 1.49445, m = 10, dimup = 30, global reward = 10, local reward = 2, penalty = \u22121, \u03b1 = 0.1, \u03b3 = 0.95, \u03b50 = 0.6, des = 0.001 4.1. An Analysis of the Role of Q-Learning To explore the role that Q-learning plays in the RLPSO algorithm, we set some pa- rameters, such as \u03b50 = 1 and des = 0, while keeping other parameters constant, to evaluate performance. Under these conditions, the RLPSO algorithm randomly selects learned particles without considering the Q table. We denote this configuration of the RLPSO algo- rithm as \u201cRandom RLPSO\u201d. The performance of both \u201cRandom RLPSO\u201d and \u201cRLPSO\u201d on 16 benchmark function problems is depicted in Figure 6, while corresponding experimental results are presented in Table 3. (a) f1: sphere model (b) f2: Schwefel\u2019s problem 2 (c) f3: Schwefel\u2019s problem (d) f4: Schwefel\u2019s problem 2 (e) f5: generalized Rosenbrock\u2019s function (f) f6: step function (g) f7: quartic function i.e.,noise (h) f8: generalized Schwefel\u2019s problem 2.26 (i) f9: generalized Rastrigin\u2019s function Figure 6. Cont. Symmetry 2024, 16, 1290 14 of 27 (j) f10: Ackley\u2019s function (k) f11: generalized griewank function (l) f12: generalized penalized function (m) f13: generalized penalized function (n) f14: six-hump camel-back function (o) f15: Branin function (p) f16: Goldstein\u2013Price function Figure 6. A comparison of Random RLPSO and RLPSO on the convergence of f 1\u2013f 16. Figure 6 illustrates that \u201cRLPSO\u201d achieves earlier convergence compared to \u201cRandom RLPSO\u201d across almost all 16 benchmark functions. Additionally, according to the experi- mental results in Table 3, \u201cRLPSO\u201d outperforms \u201cRandom RLPSO\u201d in terms of convergence for eight benchmark functions (f 3\u2013f 5, f 7, f 10\u2013f 13). As for the remaining eight benchmark functions (f 1\u2013f 2, f 6, f", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 22, "text": "RLPSO\u201d across almost all 16 benchmark functions. Additionally, according to the experi- mental results in Table 3, \u201cRLPSO\u201d outperforms \u201cRandom RLPSO\u201d in terms of convergence for eight benchmark functions (f 3\u2013f 5, f 7, f 10\u2013f 13). As for the remaining eight benchmark functions (f 1\u2013f 2, f 6, f 8\u2013f 9, f 14\u2013f 16), both \u201cRLPSO\u201d and \u201cRandom RLPSO\u201d converge to optimal values, but \u201cRLPSO\u201d does so sooner. The above analysis demonstrates that Q-learning can enhance the RLPSO algorithm\u2019s fitness value by accelerating convergence at appropriate intervals, thereby achieving a better balance between convergence speed and diversity. Strategic learning proves to be more efficient than random learning, as evidenced by experimental results. Initially, random learning with a certain probability of \u03b5k allows for comprehensive exploration and utilization of population diversity. As \u03b5k decreases, particles gradually learn from superior particles based on the Q table, further expediting convergence. The adjustment of parameters \u03b50 and des enables control over the timing of convergence acceleration. With a maximum of 5000 iterations, we aim for enhanced convergence speed after 2000 iterations (about 40% of the total) by setting parameters to \u03b50 = 0.6 and des = 0.001. Consequently, \u03b5k becomes very small (\u03b5k < 0.078) after 2000 iterations. 4.2. Parameter Setting of RLPSO The configurations of global reward, local reward, and penalty are crucial in deter- mining the performance of the RLPSO algorithm. This section outlines an experimental approach for configuring these parameters. We evaluated the performance of five sets of parameter configurations across bench- mark functions. Table 4 presents these parameter sets and compares their performance Symmetry 2024, 16, 1290 15 of 27 across 16 benchmark functions. Table 4 also includes the mean and standard deviation of the optimal solutions, with the best results among the five algorithms highlighted in bold. Table", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 23, "text": "across bench- mark functions. Table 4 presents these parameter sets and compares their performance Symmetry 2024, 16, 1290 15 of 27 across 16 benchmark functions. Table 4 also includes the mean and standard deviation of the optimal solutions, with the best results among the five algorithms highlighted in bold. Table 3. Comparisons of values extracted by the PSO, CLPSO, ELPSO, and RLPSO algorithms in 30 trials. Function PSO CLPSO ELPSO RLPSO Random RLPSO t-Test s Value f 1 Mean 6.75 \u00d7 10\u221296 4.46 \u00d7 10\u221214 4.8 \u00d7 10\u221293 0 0 1 Std 3.5 \u00d7 10\u2212100 1.73 \u00d7 10\u221214 1.26 \u00d7 10\u221293 0 0 f 2 Mean 5.53 \u00d7 10\u221216 3.79 \u00d7 10\u221212 1.41 \u00d7 10\u221212 0 0 1 Std 1.72 \u00d7 10\u221220 2.19 \u00d7 10\u221212 2.51 \u00d7 10\u221212 0 0 f 3 Mean 6.98 \u00d7 10\u22129 4.68 \u00d7 10\u22123 8.45 \u00d7 10\u221212 3.77 \u00d7 10\u2212209 4.71 \u00d7 10\u2212185 1 Std 1.25 \u00d7 10\u221210 3.83 \u00d7 10\u22123 6.58 \u00d7 10\u221212 2.02 \u00d7 10\u2212208 1.93 \u00d7 10\u2212184 f 4 Mean 1.52 \u00d7 10\u22126 2.6 2.71 \u00d7 10\u22127 1.91 \u00d7 10\u2212214 2.2 \u00d7 10\u2212207 1 Std 1.84 \u00d7 10\u22127 2.4 2.81 \u00d7 10\u22127 7.2 \u00d7 10\u2212214 7.15 \u00d7 10\u2212207 f 5 Mean 1.01 \u00d7 101 2.1 \u00d7 101 9.82 6.67 \u00d7 10\u221230 1.17 \u00d7 10\u221229 1 Std 1.66 2.98 1.66 1.49 \u00d7 10\u221229 1.37 \u00d7 10\u221229 f 6 Mean 0 0 0 0 0 0 Std 0 0 0 0 0 f 7 Mean 7.49 \u00d7 10\u22123 5.78 \u00d7 10\u22123 3.9 \u00d7 10\u22123 1.97 \u00d7 10\u22123 2.4 \u00d7 10\u22123 1 Std 9.4 \u00d7 10\u22124 2.34 \u00d7 10\u22123 1.42 \u00d7 10\u22123 8.1 \u00d7 10\u22124 1.09 \u00d7 10\u22123 f 8 Mean \u22128.44 \u00d7 103 \u22129.54 \u00d7 103 \u22121.22 \u00d7 104 \u22121.25 \u00d7 104 \u22121.25 \u00d7 104 1 Std 5.68 \u00d7 102 2.15 \u00d7 102 3.29", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 24, "text": "1.97 \u00d7 10\u22123 2.4 \u00d7 10\u22123 1 Std 9.4 \u00d7 10\u22124 2.34 \u00d7 10\u22123 1.42 \u00d7 10\u22123 8.1 \u00d7 10\u22124 1.09 \u00d7 10\u22123 f 8 Mean \u22128.44 \u00d7 103 \u22129.54 \u00d7 103 \u22121.22 \u00d7 104 \u22121.25 \u00d7 104 \u22121.25 \u00d7 104 1 Std 5.68 \u00d7 102 2.15 \u00d7 102 3.29 \u00d7 102 9.01 \u00d7 101 8.5 \u00d7 101 f 9 Mean 4.69 \u00d7 101 4.85 \u00d7 10\u221210 0 0 0 0 Std 1.59 \u00d7 101 3.63 \u00d7 10\u221210 0 0 0 f 10 Mean 1.21 0 0 1.47 \u00d7 10\u221214 1.8 \u00d7 10\u221214 1 Std 8.6 \u00d7 10\u22121 0 0 4.1 \u00d7 10\u221215 5.2 \u00d7 10\u221215 f 11 Mean 2.88 \u00d7 10\u22122 3.14 \u00d7 10\u221210 0 1.82 \u00d7 10\u22122 2.62 \u00d7 10\u22122 1 Std 3.18 \u00d7 10\u22122 4.64 \u00d7 10\u221210 0 2.36 \u00d7 10\u22122 3.01 \u00d7 10\u22122 f 12 Mean 3.52 1.12 \u00d7 10\u221211 3.02 \u00d7 10\u221217 0 0 1 Std 5.2 \u00d7 10\u22121 1.12 \u00d7 10\u221210 1.35 \u00d7 10\u221218 0 0 f 13 Mean 8.46 \u00d7 101 1.07 \u00d7 10\u221211 2.88 \u00d7 10\u221217 0 3.66 \u00d7 10\u22124 1 Std 2.35 \u00d7 10\u22121 1.7 \u00d7 10\u221227 2.06 \u00d7 10\u221222 0 1.97 \u00d7 10\u22123 f 14 Mean \u22121.0316285 \u22121.0316285 \u22121.0316285 \u22121.0316285 \u22121.0316285 0 Std 0 0 0 0 4.44 \u00d7 10\u221216 f 15 Mean 0.398903 0.398903 0.398874 0.397887 0.397887 1 Std 0 0 0 0 0 f 16 Mean 3.00 3.00 3.00 3.00 3.00 0 Std 0 0 0 0 1.22 \u00d7 10\u221215 Note: a t-test s value of 1 indicates statistically significant differences in performances between the RLPSO algorithm and the PSO, CLPSO, and ELPSO algorithms at a 95% confidence level, while a t-test s value of 0 suggests no statistically significant differences. The best results among the five algorithms highlighted in bold. The results indicate that the RLPSO algorithm", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 25, "text": "statistically significant differences in performances between the RLPSO algorithm and the PSO, CLPSO, and ELPSO algorithms at a 95% confidence level, while a t-test s value of 0 suggests no statistically significant differences. The best results among the five algorithms highlighted in bold. The results indicate that the RLPSO algorithm performs best when Global Reward = 10, Local Reward = 2 and Penalty = \u22121. In this scenario, the RLPSO algo- rithm outperforms others in 13 benchmark functions, expect for f 3, f 10, and f 11. When Global Reward = 10 and Penalty = \u22121, if gbest is updated, the particle receives Global Re- ward and follows the same learning strategy at least 10 times. Similarly, if pbest is updated, the particle receives Local Reward and follows the same learning strategy at least two times. If gbest and pbest remain unchanged for an extended period, the particle adjusts its learning strategy and begins to learn from other particles\u2019 pbests. The RLPSO algorithm balances convergence speed and particle diversity through reward and penalty. Therefore, setting Global Reward = 10, Local Reward = 2, and Penalty = \u22121 achieves a desirable balance between convergence speed and particle diversity. Additionally, we set \u03b1 = 0.1 and \u03b3 = 0.95 following the conventional Q-learning parameter settings of many cases. Symmetry 2024, 16, 1290 16 of 27 Table 4. Comparisons of 5 groups of parameter settings. Function Global Reward = 10 Local Reward = 5 Penalty = \u22121 Global Reward = 10 Local Reward = 5 Penalty = \u22122 Global Reward = 10 Local Reward = 2 Penalty = \u22121 Global Reward = 10 Local Reward = 2 Penalty = \u22122 Global Reward = 10 Local Reward = 1 Penalty = \u22122 f 1 Mean 0 0 0 0 0 Std 0 0 0", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 26, "text": "= 5 Penalty = \u22122 Global Reward = 10 Local Reward = 2 Penalty = \u22121 Global Reward = 10 Local Reward = 2 Penalty = \u22122 Global Reward = 10 Local Reward = 1 Penalty = \u22122 f 1 Mean 0 0 0 0 0 Std 0 0 0 0 0 f 2 Mean 0 0 0 0 0 Std 0 0 0 0 0 f 3 Mean 8.62 \u00d7 10\u2212197 4.3 \u00d7 10\u2212218 3.77 \u00d7 10\u2212209 2.6 \u00d7 10\u2212201 1.15 \u00d7 10\u2212194 Std 4.64 \u00d7 10\u2212196 2.31 \u00d7 10\u2212217 2.02 \u00d7 10\u2212208 1.4 \u00d7 10\u2212200 5.52 \u00d7 10\u2212194 f 4 Mean 1.3 \u00d7 10\u2212204 2.99 \u00d7 10\u2212209 1.91 \u00d7 10\u2212214 3.39 \u00d7 10\u2212210 5.13 \u00d7 10\u2212206 Std 6.99 \u00d7 10\u2212204 8.97 \u00d7 10\u2212209 7.2 \u00d7 10\u2212214 1.82 \u00d7 10\u2212209 2.53 \u00d7 10\u2212205 f 5 Mean 1.53 \u00d7 10\u221229 2.25 \u00d7 10\u221229 6.67 \u00d7 10\u221230 1.05 \u00d7 10\u221229 1.01 \u00d7 10\u221229 Std 2.28 \u00d7 10\u221229 7.23 \u00d7 10\u221229 1.49 \u00d7 10\u221229 2.09 \u00d7 10\u221229 1.57 \u00d7 10\u221229 f 6 Mean 0 0 0 0 0 Std 0 0 0 0 0 f 7 Mean 2.47 \u00d7 10\u22123 2.21 \u00d7 10\u22123 1.97 \u00d7 10\u22123 2.43 \u00d7 10\u22123 2.51 \u00d7 10\u22123 Std 9.26 \u00d7 10\u22124 1.43 \u00d7 10\u22123 8.1 \u00d7 10\u22124 1.06 \u00d7 10\u22123 9.22 \u00d7 10\u22124 f 8 Mean \u22121.25 \u00d7 104 \u22121.25 \u00d7 104 \u22121.25 \u00d7 104 \u22121.25 \u00d7 104 \u22121.25 \u00d7 104 Std 6.23 \u00d7 101 7.95 \u00d7 101 9.01 \u00d7 101 6.23 \u00d7 101 6.23 \u00d7 101 f 9 Mean 0 0 0 3.32 \u00d7 10\u22122 0 Std 0 0 0 1.79 \u00d7 10\u22121 0 f 10 Mean 1.44 \u00d7 10\u221214 1.43 \u00d7 10\u221214 1.47 \u00d7 10\u221214 1.41 \u00d7 10\u221214 1.38 \u00d7 10\u221214 Std 3.99 \u00d7 10\u221215 2.95 \u00d7 10\u221215 4.1 \u00d7 10\u221215 2.76 \u00d7 10\u221215 3.86", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 27, "text": "f 9 Mean 0 0 0 3.32 \u00d7 10\u22122 0 Std 0 0 0 1.79 \u00d7 10\u22121 0 f 10 Mean 1.44 \u00d7 10\u221214 1.43 \u00d7 10\u221214 1.47 \u00d7 10\u221214 1.41 \u00d7 10\u221214 1.38 \u00d7 10\u221214 Std 3.99 \u00d7 10\u221215 2.95 \u00d7 10\u221215 4.1 \u00d7 10\u221215 2.76 \u00d7 10\u221215 3.86 \u00d7 10\u221215 f 11 Mean 2.12 \u00d7 10\u22122 1.81 \u00d7 10\u22122 1.82 \u00d7 10\u22122 1.52 \u00d7 10\u22122 2.29 \u00d7 10\u22122 Std 2.4 \u00d7 10\u22122 2.08 \u00d7 10\u22122 2.36 \u00d7 10\u22122 1.91 \u00d7 10\u22122 2.56 \u00d7 10\u22122 f 12 Mean 0 0 0 0 0 Std 0 0 0 0 0 f 13 Mean 0 0 0 0 0 Std 0 0 0 0 0 f 14 Mean \u22121.0316285 \u22121.0316285 \u22121.0316285 \u22121.0316285 \u22121.0316285 Std 0 9.57 \u00d7 10\u221216 0 0 9.62 \u00d7 10\u221215 f 15 Mean 0.397887 0.397887 0.397887 0.397887 0.397887 Std 0 3.19 \u00d7 10\u221216 0 0 4.66 \u00d7 10\u221214 f 16 Mean 3.00 3.00 3.00 3.00 3.00 Std 0 0 0 0 0 Note: the best results among the five algorithms highlighted in bold. 4.3. Experimental Results and Analysis In this section, we compare the RLPSO algorithm and the Random RLPSO algorithm with classical PSO, CLPSO, and ELPSO algorithms. Table 3 presents a comparison of the above five algorithms in 16 benchmark functions. The table displays the mean and standard deviation of the optimal solutions for the PSO, CLPSO, ELPSO, Random RLPSO, and RLPSO algorithms, with the best results among the five algorithms highlighted in bold. The results of the t-test are presented in the last column of Table 3. At a 95% confidence level, when s = 1, this indicates that the performance differences between the RLPSO algo- rithm and the PSO, CLPSO, and ELPSO algorithms are statistically significant. Conversely, when s = 0, it suggests no statistically", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 28, "text": "the t-test are presented in the last column of Table 3. At a 95% confidence level, when s = 1, this indicates that the performance differences between the RLPSO algo- rithm and the PSO, CLPSO, and ELPSO algorithms are statistically significant. Conversely, when s = 0, it suggests no statistically significant differences. Among the comparisons for the 16 benchmark functions in Table 3, 12 exhibited statistically significant disparities. As shown in Table 3, the RLPSO algorithm demonstrates superior performance in solving unimodal function problems (f 1\u2013f 5, f 7) compared to all other algorithms. When tackling multimodal function problems (f 8\u2013f 16), RLPSO outperforms in seven cases (f 8, f 9, f 12\u2013 f 16). Overall, the RLPSO algorithm emerges as the best solution in 14 out of 16 function problems. Despite the RLPSO algorithm\u2019s average best solution value being lower than that of the CLPSO and ELPSO algorithms in the multimodal function problem f 11, RLPSO successfully identifies 12 global optimal solutions of f 11 across 30 runs. To provide a comprehensive assessment of the RLPSO algorithm, Figure 7 illustrates the frequency with which RLPSO finds the global optimum in 30 runs across 16 benchmark functions. Symmetry 2024, 16, 1290 17 of 27 30 30 0 0 20 30 0 17 30 0 12 30 30 30 30 30 0 10 20 30 40 f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12 f13 f14 f15 f16 The times of finding the global minimum by the RLPSO algorithm Figure 7. The times of finding the global minimum by the RLPSO algorithm. Figure 7 shows that when the RLPSO algorithm is applied to each benchmark function problem 30 times, it successfully identifies 12 global minima for 16 benchmark function problems. In the case of the remaining 4", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 29, "text": "RLPSO algorithm Figure 7. The times of finding the global minimum by the RLPSO algorithm. Figure 7 shows that when the RLPSO algorithm is applied to each benchmark function problem 30 times, it successfully identifies 12 global minima for 16 benchmark function problems. In the case of the remaining 4 benchmark function problems (f 3\u2013 f 4, f 7, f 10) where the global minimum is not found, the gbests discovered by the RLPSO algorithm are very close to the global optimum. For instance, the average value of the optimal solution for f 3 is 3.77 \u00d7 10\u2212209, which is close to 0, the global optimal solution of f 3. Similarly, the average value of the optimal solution for f 4 is 1.91 \u00d7 10\u2212214, also close to 0, the global optimal solution of f 4. The same applies to f 7 and f 10. Based on the above analysis, we can conclude that the RLPSO algorithm has the capability to identify the global optimum when run multiple times within a certain error range. Furthermore, it is evident that various PSO algorithms exhibit different performances across different benchmark functions. At times, they may successfully locate the global optimum, while in other instances, they may become trapped in local optima or converge too slowly. The experimental results confirm that the RLPSO algorithm manages to obtain nearly all global optima within a certain margin of error. Consequently, the RLPSO algo- rithm demonstrates greater stability compared to other PSO algorithms when addressing diverse problem sets. To further validate the performance of the RLPSO algorithm, we selected 13 test functions from the CEC2017 benchmark set as the benchmark functions and compared the RLPSO algorithm with four PSO algorithms and two evolutionary algorithms on the test set. To evaluate the RLPSO algorithm\u2019s performance, we compared", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 30, "text": "problem sets. To further validate the performance of the RLPSO algorithm, we selected 13 test functions from the CEC2017 benchmark set as the benchmark functions and compared the RLPSO algorithm with four PSO algorithms and two evolutionary algorithms on the test set. To evaluate the RLPSO algorithm\u2019s performance, we compared it with other algorithms using identical test parameters. All algorithms were configured with the same settings: a population size of 50, a maximum iteration count of 1000, a particle dimension of 30, and each algorithm was executed 50 times [24]. The selected test functions are listed in Table 5, algorithm parameters are provided in Table 6, and the computational results are presented in Table 7. The data from Table 7 clearly indicate that the RLPSO algorithm outperforms the other six algorithms (CLPSO, HPSO, THSPSO, TCSPSO, BOA, and OSA algorithms) in the testing of the 13 benchmark functions of CEC2017, achieving the minimum value on 11 of these benchmarks. This demonstrates the superior performance of the RLPSO algorithm, suggesting its capability to find solutions closer to the global optimum in most scenarios. These results imply that the RLPSO algorithm could be an effective and reliable choice for addressing complex optimization problems. 4.4. Particle Swarm Diversity Analysis We conducted Principal Component Analysis (PCA) on functions f 17 to f 29 over 1000 iterations, tracking the positions of 50 particles at the 200th, 400th, 600th, and 800th iterations. To evaluate particle diversity during the iterative process, we utilized PCA Symmetry 2024, 16, 1290 18 of 27 to reduce the 30-dimensional particle position data to 3 dimensions. This analysis of the principal components offered valuable insights into particle behavior and enhanced swarm optimization. The findings are illustrated in Figures 8\u201320. Table 5. CEC2017 benchmark functions. NO. Function D Range fopt f 17 Shifted and", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 31, "text": "18 of 27 to reduce the 30-dimensional particle position data to 3 dimensions. This analysis of the principal components offered valuable insights into particle behavior and enhanced swarm optimization. The findings are illustrated in Figures 8\u201320. Table 5. CEC2017 benchmark functions. NO. Function D Range fopt f 17 Shifted and Rotated Zakharov 30 [\u2212100,100] 300 f 18 Shifted and Rotated Rastrigin 30 [\u2212100,100] 500 f 19 Shifted and Rotated Lunacek Bi-Rastrigin 30 [\u2212100,100] 700 f 20 Shifted and Rotated Non-Continuous Rastrigin 30 [\u2212100,100] 800 f 21 Shifted and Rotated Schwefel 30 [\u2212100,100] 1000 f 22 Hybrid Function 2 (N = 3) 30 [\u2212100,100] 1200 f 23 Hybrid Function 4 (N = 4) 30 [\u2212100,100] 1400 f 24 Hybrid Function 5 (N = 4) 30 [\u2212100,100] 1500 f 25 Hybrid Function 6 (N = 4) 30 [\u2212100,100] 1600 f 26 Composition Function 1 (N = 3) 30 [\u2212100,100] 2100 f 27 Composition Function 2 (N = 3) 30 [\u2212100,100] 2200 f 28 Composition Function 5 (N = 5) 30 [\u2212100,100] 2500 f 29 Composition Function 6 (N = 5) 30 [\u2212100,100] 2600 Table 6. Some variants of PSO and other compared evolutionary algorithms. Algorithm Years Parameter Information CLPSO [13] 2006 \u03c9 = 0.9~0.4, c = 1.49445, m = 7. HPSO [29] 2014 \u03c9 = 0.9~0.4, c1 = c2 = 2, c3 = randn [0,1]. THSPSO [30] 2019 c1 = c2 = c3 = 2, \u03c9 = 0.9 TCSPSO [24] 2019 \u03c9 = 0.9~0.4, c1 = c2 = 2 BOA [31] 2018 Sensormodality c = 0~1, Powerexponent: 0.1~0.3, Switchprobability: p = 0~8 OSA [32] 2018 \u03b2 = 1.9~0 Figure 8. PCA was applied to the positions of 50 particles in f 17 of CEC2017 for Shifted and Rotated Zakharov. Note: The colored dots are the positions of the particles. Figure 9. PCA", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 32, "text": "Sensormodality c = 0~1, Powerexponent: 0.1~0.3, Switchprobability: p = 0~8 OSA [32] 2018 \u03b2 = 1.9~0 Figure 8. PCA was applied to the positions of 50 particles in f 17 of CEC2017 for Shifted and Rotated Zakharov. Note: The colored dots are the positions of the particles. Figure 9. PCA was applied to the positions of 50 particles in f 18 of CEC2017 for Shifted and Rotated Rastrigin. Note: The colored dots are the positions of the particles. Symmetry 2024, 16, 1290 19 of 27 Table 7. Comparisons of values extracted by the CLPSO, HPSO, THSPSO, TCSPSO, BOA, and OSA algorithms in 50 trials. Function CLPSO HPSO THSPSO TCSPSO BOA OSA RLPSO t-Test s Value f 17 Mean 9.17 \u00d7 104 4.57 \u00d7 104 8.07 \u00d7 104 2.2 \u00d7 104 8.23 \u00d7 104 9.16 \u00d7 104 3.13 \u00d7 102 1 Std 1.46 \u00d7 104 1.75 \u00d7 104 4.45 \u00d7 103 4.72 \u00d7 103 7.23 \u00d7 103 2.79 \u00d7 103 1.75 \u00d7 101 f 18 Mean 6.57 \u00d7 102 6.12 \u00d7 102 8.99 \u00d7 102 6.02 \u00d7 102 8.99 \u00d7 102 9.38 \u00d7 102 5 \u00d7 102 1 Std 1.05 \u00d7 101 5.04 \u00d7 101 3.8 \u00d7 101 2.54 \u00d7 101 2.59 \u00d7 101 2.35 \u00d7 101 7.36\u00d7 10\u22124 f 19 Mean 9.53 \u00d7 102 8.58 \u00d7 102 1.38 \u00d7 103 8.51 \u00d7 102 1.36 \u00d7 103 1.47 \u00d7 103 9.06 \u00d7 102 1 Std 1.64 \u00d7 101 4.07 \u00d7 101 5.56 \u00d7 101 3.98 \u00d7 101 4.04 \u00d7 101 4.32 \u00d7 101 5.96 \u00d7 101 f 20 Mean 9.6 \u00d7 102 9.01 \u00d7 102 1.13 \u00d7 103 8.91 \u00d7 102 1.13 \u00d7 103 1.15 \u00d7 103 8.09 \u00d7 102 1 Std 1.26 \u00d7 101 5.11 \u00d7 101 2.93 \u00d7 101 2.53 \u00d7 101 1.95 \u00d7 101 2.43 \u00d7 101 5.18", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 33, "text": "\u00d7 101 5.96 \u00d7 101 f 20 Mean 9.6 \u00d7 102 9.01 \u00d7 102 1.13 \u00d7 103 8.91 \u00d7 102 1.13 \u00d7 103 1.15 \u00d7 103 8.09 \u00d7 102 1 Std 1.26 \u00d7 101 5.11 \u00d7 101 2.93 \u00d7 101 2.53 \u00d7 101 1.95 \u00d7 101 2.43 \u00d7 101 5.18 f 21 Mean 6.26 \u00d7 103 8.05 \u00d7 103 8.75 \u00d7 103 4.85 \u00d7 103 8.84 \u00d7 103 9.04 \u00d7 103 1.31 \u00d7 103 1 Std 2.88 \u00d7 102 5.99 \u00d7 102 5.95 \u00d7 102 9.05 \u00d7 102 3.11 \u00d7 102 4.36 \u00d7 102 3.14 \u00d7 102 f 22 Mean 1.16 \u00d7 107 8.87 \u00d7 105 1.07 \u00d7 1010 3.18 \u00d7 106 1.27 \u00d7 1010 1.44 \u00d7 1010 4.18 \u00d7 103 1 Std 3.54 \u00d7 106 9.07 \u00d7 105 2.88 \u00d7 109 4.1 \u00d7 106 3.34 \u00d7 109 2.16 \u00d7 109 3.03 \u00d7 103 f 23 Mean 6.13 \u00d7 104 6.8 \u00d7 104 1.56 \u00d7 106 6.6 \u00d7 104 1.22 \u00d7 107 2.14 \u00d7 107 3.83 \u00d7 103 1 Std 4.41 \u00d7 104 5.24 \u00d7 104 9.77 \u00d7 105 1.02 \u00d7 105 2.09 \u00d7 107 2.03 \u00d7 107 2.16 \u00d7 103 f 24 Mean 3.72 \u00d7 104 1.03 \u00d7 104 1.48 \u00d7 108 9.3 \u00d7 103 6.83 \u00d7 108 8.28 \u00d7 108 1.63 \u00d7 103 1 Std 1.84 \u00d7 104 1 \u00d7 104 1.52 \u00d7 108 8.85 \u00d7 103 4.37 \u00d7 108 3.43 \u00d7 108 1.88 \u00d7 102 f 25 Mean 2.47 \u00d7 103 2.65 \u00d7 103 5.04 \u00d7 103 2.67 \u00d7 103 7.3 \u00d7 103 6.17 \u00d7 103 1.84 \u00d7 103 1 Std 1.6 \u00d7 102 3.51 \u00d7 102 6.82 \u00d7 102 3.1 \u00d7 102 1.39 \u00d7 103 9.21 \u00d7 102 2.92 \u00d7 102 f 26 Mean 2.44 \u00d7 103 2.41 \u00d7 103 2.71 \u00d7 103 2.41 \u00d7 103", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 34, "text": "2.67 \u00d7 103 7.3 \u00d7 103 6.17 \u00d7 103 1.84 \u00d7 103 1 Std 1.6 \u00d7 102 3.51 \u00d7 102 6.82 \u00d7 102 3.1 \u00d7 102 1.39 \u00d7 103 9.21 \u00d7 102 2.92 \u00d7 102 f 26 Mean 2.44 \u00d7 103 2.41 \u00d7 103 2.71 \u00d7 103 2.41 \u00d7 103 2.65 \u00d7 103 2.76 \u00d7 103 2.24 \u00d7 103 1 Std 3.7 \u00d7 101 5.12 \u00d7 101 5.49 \u00d7 101 2.77 \u00d7 101 1.23 \u00d7 102 4.56 \u00d7 101 2.71 \u00d7 101 f 27 Mean 2.94 \u00d7 103 7.04 \u00d7 103 8.2 \u00d7 103 2.74 \u00d7 103 5.44 \u00d7 103 1.01 \u00d7 104 2.44 \u00d7 103 1 Std 1.07 \u00d7 103 3.2 \u00d7 103 1.29 \u00d7 103 1.36 \u00d7 103 1.08 \u00d7 103 6.9 \u00d7 102 9.51 \u00d7 101 f 28 Mean 2.93 \u00d7 103 2.89 \u00d7 103 4.43 \u00d7 103 2.94 \u00d7 103 5.64 \u00d7 103 4.71 \u00d7 103 3.32 \u00d7 103 1 Std 9.74 1.45 3.71 \u00d7 102 2.57 \u00d7 101 5.27 \u00d7 102 3.61 \u00d7 102 2.65 \u00d7 101 f 29 Mean 4.81 \u00d7 103 4.99 \u00d7 103 1.01 \u00d7 104 4.7 \u00d7 103 1.14 \u00d7 104 1.14 \u00d7 104 3.14 \u00d7 103 1 Std 4.67 \u00d7 102 5.5 \u00d7 102 6.96 \u00d7 102 1.19 \u00d7 103 9.03 \u00d7 102 8.81 \u00d7 102 8.63 Note: a t-test s value of 1 indicates statistically significant differences in performances between the RLPSO algorithm and any other algorithm at a 95% confidence level, while a t-test s value of 0 suggests no statistically significant differences. Figure 10. PCA was applied to the positions of 50 particles in f 19 of CEC2017 for Shifted and Rotated Lunacek Bi-Rastrigin. Note: The colored dots are the positions of the particles. Figure 11. PCA was applied to the positions of 50 particles in f", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 35, "text": "suggests no statistically significant differences. Figure 10. PCA was applied to the positions of 50 particles in f 19 of CEC2017 for Shifted and Rotated Lunacek Bi-Rastrigin. Note: The colored dots are the positions of the particles. Figure 11. PCA was applied to the positions of 50 particles in f 20 of CEC2017 for Shifted and Rotated Non-Continuous Rastrigin. Note: The colored dots are the positions of the particles. Symmetry 2024, 16, 1290 20 of 27 Figure 12. PCA was applied to the positions of 50 particles in f 21 of CEC2017 for Shifted and Rotated Schwefel. Note: The colored dots are the positions of the particles. Figure 13. PCA was applied to the positions of 50 particles in f 22 of CEC2017 for Hybrid Function 2 (N = 3). Note: The colored dots are the positions of the particles. Figure 14. PCA was applied to the positions of 50 particles in f 23 of CEC2017 for Hybrid Function 4 (N = 4). Note: The colored dots are the positions of the particles. Figure 15. PCA was applied to the positions of 50 particles in f 24 of Hybrid Function 5 (N = 4). Note: The colored dots are the positions of the particles. Symmetry 2024, 16, 1290 21 of 27 Figure 16. PCA was applied to the positions of 50 particles in f 25 of Hybrid Function 6 (N = 4). Note: The colored dots are the positions of the particles. Figure 17. PCA was applied to the positions of 50 particles in f 26 of Composition Function 1 (N = 3). Note: The colored dots are the positions of the particles. Figure 18. PCA was applied to the positions of 50 particles in f 27 of Composition Function 2 (N = 3). Note: The colored dots are the", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 36, "text": "of 50 particles in f 26 of Composition Function 1 (N = 3). Note: The colored dots are the positions of the particles. Figure 18. PCA was applied to the positions of 50 particles in f 27 of Composition Function 2 (N = 3). Note: The colored dots are the positions of the particles. Figure 19. PCA was applied to the positions of 50 particles in f 28 of Composition Function 5 (N = 5). Note: The colored dots are the positions of the particles. Symmetry 2024, 16, 1290 22 of 27 Figure 20. PCA was applied to the positions of 50 particles in f 29 of Composition Function 6 (N = 5). Note: The colored dots are the positions of the particles. From Figures 8\u201320, it is evident that the algorithm continues to improve beyond 600 function iterations and does not reach convergence. For certain functions (f 17, f 18, f 25, f 26, f 28, f 29), convergence is not achieved even after 800 function evaluations. Furthermore, the comparisons across different runs highlight the algorithm\u2019s stability for these functions. To further analyze swarm diversity, a diversity measurement [33] is considered and defined as follows: divj = 1 N N \u2211 i=1 median(xj) \u2212xj i (9) div = 1 d d \u2211 j=1 divj (10) N and d represent the number of particles and dimensions, respectively, xj i represents the j\u2019th dimension of the i\u2019th particle, while the median (xj) is the median of dimension j in the whole swarm, divj is the diversity in each dimension, the diversity of whole population (div) is calculated by averaging all divj. Furthermore, with the help of diversity measurement, we can calculate the percentage of exploration and exploitation during each iteration using the following equations: exploration% = ( div divmax ) \u00d7", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 37, "text": "swarm, divj is the diversity in each dimension, the diversity of whole population (div) is calculated by averaging all divj. Furthermore, with the help of diversity measurement, we can calculate the percentage of exploration and exploitation during each iteration using the following equations: exploration% = ( div divmax ) \u00d7 100% (11) exploitation% = 1 \u2212exploration% (12) divmax is defined as the maximum diversity value achieved throughout the optimiza- tion process. The exploration% connects the diversity in each iteration to this maximum diversity. It is inversely related to the exploitation level and is calculated as the comple- mentary percentage to exploitation%. We conducted research on the percentage of exploration and exploitation in 1000 iterations for f 17\u2013 f 29. The results are shown in Figure 21. From Figure 21, we can observe that the exploitation% of the algorithm starts to increase around the 400th iteration. For f 20 and f 23, when the number of iterations reaches 800, the exploitation% nearly approaches 100%. This observation is consistent with what is shown in Figures 11 and 14, where the particles are in a clustered state. The data in Figure 21 indicate that as the number of iterations increases, the algorithm\u2019s performance gradually improves and eventually stabilizes. This suggests that in the later stages of the algorithm\u2019s execution, the clustering of particles significantly enhances the exploitation rate, reaching nearly full exploitation. Moreover, the analysis of the percentages of exploration and exploitation reveals that the algorithm predominantly explores in the early stages, with exploration% reaching nearly 100%. In the later stages, the algorithm shifts towards exploiting the accumulated information to accelerate convergence. This behavior is consistent across different problems, Symmetry 2024, 16, 1290 23 of 27 highlighting the algorithm\u2019s stability and its ability to effectively balance exploration and exploitation in various scenarios. 4.5.", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 38, "text": "exploration% reaching nearly 100%. In the later stages, the algorithm shifts towards exploiting the accumulated information to accelerate convergence. This behavior is consistent across different problems, Symmetry 2024, 16, 1290 23 of 27 highlighting the algorithm\u2019s stability and its ability to effectively balance exploration and exploitation in various scenarios. 4.5. Engineering Problem In this section, the RLPSO algorithm was applied to solve the three-bar truss prob- lem [33], and the maximum iteration and population size are 1000 and 30, respectively. The mathematical formulations are as follows: \u2192x = (x1, x2) (13) Objective function: Min.f (x) = L \u2217(x2 + 2 \u221a 2x1) (14) These are subject to the following: h1(x) = x2 2x2x1 + \u221a 2x2 1 P \u2212\u03b4 \u22640, (15) h2(x) = x2 + \u221a 2x1 2x2x1 + \u221a 2x2 1 P \u2212\u03b4 \u22640, (16) h3(x) = 1 x1 + \u221a 2x2 P \u2212\u03b4 \u22640, (17) where 0 \u2264x1, x2 \u22641, and P = 2, L = 100, and \u03b4 = 2. This engineering problem is solved by using our proposed RLPSO algorithm and compared with the methods mentioned in reference [33]. The results of the comparison are shown in Table 8. As shown in Table 8, RLPSO performs well in solving the engineering problem. However, the complexity of the boundary conditions in this engineering problem often leads to particles not satisfying the constraints after updating, resulting in ineffective learning. Additionally, due to the problem\u2019s low dimensionality, particles lose a significant amount of their inherent information after updating, resulting in substantial changes in particle positions and the loss of valuable data. Consequently, addressing practical engineering problems with complex constraint conditions will be a major focus of our future research. Table 8. Comparison performance of RLPSO with other algorithms for three-bar truss problem. Algorithm Optimal Weight RLPSO 209.173475679612 m-DMFO 174.2761613819025", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 39, "text": "in substantial changes in particle positions and the loss of valuable data. Consequently, addressing practical engineering problems with complex constraint conditions will be a major focus of our future research. Table 8. Comparison performance of RLPSO with other algorithms for three-bar truss problem. Algorithm Optimal Weight RLPSO 209.173475679612 m-DMFO 174.2761613819025 MFO 263.895979682 DEDS 263.8958434 MBA 263.8958522 Tsa 263.68 PSO-DE 263.8958433 CS 263.9716 Symmetry 2024, 16, 1290 24 of 27 Figure 21. exploration% and exploitation% of f 17\u2013f 29. 5. Conclusions This study explores the application of strategic learning in optimization by proposing a Reinforcement Learning-based Particle Swarm Optimization (RLPSO) algorithm aimed at improving the performance and convergence speed of traditional PSO algorithms. The research on the RLPSO algorithm involves knowledge from multiple theoretical domains, including Particle Swarm Optimization, Reinforcement Learning, Q-learning, multi-modal optimization, and adaptive algorithms. This study is based on a deep understanding of these theories and their effective integration. Symmetry 2024, 16, 1290 25 of 27 Under the same testing parameter settings, performance comparisons were conducted between the RLPSO algorithm and the PSO, CLPSO, and ELPSO algorithms on the testing of 16 benchmark functions from CEC2005. The results revealed that, compared to other algorithms, the RLPSO algorithm exhibited the fastest convergence speed. It also found the global optimum in 14 out of the 16 benchmark functions, showing significant statistical differences. In the testing of 13 benchmark functions from CEC2017, performance compar- isons were made between the RLPSO algorithm and six other algorithms (CLPSO, HPSO, THSPSO, TCSPSO, BOA, and OSA). The results demonstrated that the RLPSO algorithm found the global optimum in 11 out of the 13 benchmark functions, with statistically signif- icant differences. This indicates that the RLPSO algorithm exhibits excellent performance across multiple benchmark function problems, which finds the global optimum in almost all the cases. The", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 40, "text": "OSA). The results demonstrated that the RLPSO algorithm found the global optimum in 11 out of the 13 benchmark functions, with statistically signif- icant differences. This indicates that the RLPSO algorithm exhibits excellent performance across multiple benchmark function problems, which finds the global optimum in almost all the cases. The introduced Q-learning mechanism in reinforcement learning plays a crucial role in enhancing algorithm performance. This algorithm selects particles to update their velocities based on an online updated Q-table. At the beginning of each iteration, particles randomly choose particles to learn from with a certain probability, exploring the solution space as much as possible to update the Q-table. This process filters out particles worth learning from and stores the information in the Q-table. As iterations proceed, particles determine which particles they want to learn from based on the Q-table and store the learning results in the Q-table to guide the next step of learning. The algorithm continuously adjusts the learning targets and updates the learning strategy online to accelerate convergence speed at the right time, thus striking a good balance between convergence speed and diversity. In this paper, comparisons with random algorithms that do not incorporate Q-table learning reveal that the RLPSO algorithm converges faster. This indicates the crucial role of the Q-learning mechanism introduced in reinforcement learning in enhancing algorithm performance. By combining Q-learning with particle swarm optimization, we achieve effective learning and experience sharing among particles, accelerating the algorithm\u2019s convergence speed, and obtaining better fitness values at the appropriate times. This demonstrates the effectiveness of strategic learning compared to random learning, providing strong support for further research and the application of reinforcement learning in optimization algorithms. Compared with traditional PSO algorithms and other improved versions, the RLPSO algorithm exhibits strong adaptability, fast convergence speed, strong global search capability, ease", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 41, "text": "times. This demonstrates the effectiveness of strategic learning compared to random learning, providing strong support for further research and the application of reinforcement learning in optimization algorithms. Compared with traditional PSO algorithms and other improved versions, the RLPSO algorithm exhibits strong adaptability, fast convergence speed, strong global search capability, ease of implementation and application, and demonstrates more stable and efficient performance, making it more applicable and versatile when facing different types of optimization problems. The RLPSO algorithm has a wide range of applications, such as in engineering design, data mining, and artificial intelligence. By improving the performance and robustness of optimization algorithms, the RLPSO algorithm provides effective solutions for solving practical problems. Additionally, in terms of performance on the CEC2005 and CEC2017 test functions, the RLPSO algorithm performs excellently in handling both multi-modal and single-modal problems, and is particularly outstanding in solving multi-modal problems, indicating its applicability in solving complex problems and further validating the superiority of the RLPSO algorithm over other PSO algorithms in solving practical problems. The diver- sity graphs also reveal that the algorithm maintains the diversity of the particle swarm throughout the computation, and balances the exploration and utilization of the algorithm. Through the engineering problem verification, it shows that the algorithm has a certain practical application value. This suggests that the RLPSO algorithm has broad potential in practical applications, especially in engineering optimization and data mining fields. Despite the significant achievements of the RLPSO algorithm in optimization problems, there are limitations that need to be considered. The reinforcement learning parameters in the RLPSO algorithm need appropriate adjustments, including on learning rate, rewards, and penalties. The choice of these parameters may significantly affect the algorithm\u2019s per- formance and convergence speed, but determining the optimal parameter settings typically requires a large number of experiments and experiences. The", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 42, "text": "considered. The reinforcement learning parameters in the RLPSO algorithm need appropriate adjustments, including on learning rate, rewards, and penalties. The choice of these parameters may significantly affect the algorithm\u2019s per- formance and convergence speed, but determining the optimal parameter settings typically requires a large number of experiments and experiences. The updating and maintenance Symmetry 2024, 16, 1290 26 of 27 of the Q-table in the RLPSO algorithm increase the computational cost, especially when dealing with high-dimensional problems or large-scale optimization tasks. Therefore, prac- tical applications may face limitations in computational resources. Although the RLPSO algorithm demonstrates many advantages in optimization problems, further research and improvements are still needed to overcome its limitations and enhance the algorithm\u2019s performance and applicability. Future research can focus on optimizing the parameter settings of Q-learning in the RLPSO algorithm to further enhance the algorithm\u2019s performance and robustness. Ad- ditionally, ideas from other novel PSO algorithms can be borrowed, such as the Particle Swarm Optimization algorithm with priority-based sorting [34] and the DOADAPO al- gorithm [35], to extend Q-learning into the multi-objective optimization domain, thereby exploring a wider problem space. These research directions will contribute to a deeper understanding of the working principles of the RLPSO algorithm and further promote the application and development of reinforcement learning-based optimization algorithms in practical problems. In summary, as a reinforcement learning-based optimization algorithm, the RLPSO algorithm not only has significant theoretical significance but also has broad prospects in practical applications. We believe that through further research and exploration, the RLPSO algorithm will play a more important role in the field of optimization and provide effective solutions for practical problems. Author Contributions: F.Z. and Z.C. proposed the algorithm and were responsible for writing the article. All authors have read and agreed to the published version of the manuscript. Funding: This", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 43, "text": "RLPSO algorithm will play a more important role in the field of optimization and provide effective solutions for practical problems. Author Contributions: F.Z. and Z.C. proposed the algorithm and were responsible for writing the article. All authors have read and agreed to the published version of the manuscript. Funding: This study was supported by the National Social Science Fund of China (2022-SKJJ-B-112). Data Availability Statement: The datasets used and analyzed during the current study are available from the corresponding author on reasonable request. Acknowledgments: We would like to thank all the authors whose articles are referenced in our study. We extend special thanks to the professors of the Sixty-third Institute of the National University of Defense Technology. Conflicts of Interest: The authors declare no conflicts of interest. References 1. Kennedy, J.; Eberhart, R. Particle swarm optimization. In Proceedings of the 1995 IEEE International Conference on Neural Networks, Perth, WA, Australia, 27 November\u20131 December 1995; pp. 1942\u20131948. 2. Sabir, Z.; Raja, M.A.Z.; Umar, M.; Shoaib, M. Design of neuro-swarming-based heuristics to solve the third-order nonlinear multi-singular Emden-Fowler equation. Eur. Phys. J. Plus 2020, 135, 410. [CrossRef] 3. Che, H.; Wang, J. A Two-Timescale Duplex Neurodynamic Approach to Mixed-Integer Optimization. IEEE Trans. Neural Netw. Learn. Syst. 2021, 32, 36\u201348. [CrossRef] [PubMed] 4. Shao, H.D.; Jiang, H.K.; Zhang, X.; Niu, M.G. Rolling bearing fault diagnosis using an optimization deep belief network. Meas. Sci. Technol. 2015, 26, 115002. [CrossRef] 5. Yan, X.A.; Jia, M.P. A novel optimized SVM classification algorithm with multi-domain feature and its application to fault diagnosis of rolling bearing. Neurocomputing 2018, 313, 47\u201364. [CrossRef] 6. Mohammad, Y.; Eberhart, R.; Mohammad, H.S. A Novel Flexible Inertia Weight Particle Swarm Optimization Algorithm. PLoS ONE 2016, 11, e0161558. 7. Ratnaweera, J.A.; Mousa, S.; Watson, H.C. Self-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients.", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 44, "text": "its application to fault diagnosis of rolling bearing. Neurocomputing 2018, 313, 47\u201364. [CrossRef] 6. Mohammad, Y.; Eberhart, R.; Mohammad, H.S. A Novel Flexible Inertia Weight Particle Swarm Optimization Algorithm. PLoS ONE 2016, 11, e0161558. 7. Ratnaweera, J.A.; Mousa, S.; Watson, H.C. Self-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients. IEEE Trans. Evol. Comput. 2004, 8, 240\u2013255. [CrossRef] 8. Liu, Y.; Lu, H.; Cheng, S.; Shi, Y. An Adaptive Online Parameter Control Algorithm for Particle Swarm Optimization Based on Reinforcement Learning. In Proceedings of the 2019 IEEE Congress on Evolutionary Computation, Wellington, New Zealand, 10\u201313 June 2019; pp. 815\u2013822. 9. Liu, W.; Wang, Z.; Yuan, Y.; Zeng, N.; Hone, K.; Liu, X. A novel sigmoid-function-based adaptive weighted particle swarm optimizer. IEEE Trans. Cybern. 2021, 51, 1085\u20131093. [CrossRef] 10. Mendes, R.; Kennedy, J.; Neves, J. The fully informed particle swarm: Simpler, maybe better. IEEE Trans. Evol. Comput. 2004, 8, 204\u2013210. [CrossRef] 11. Yang, C.H.; Lin, Y.S.; Chang, L.Y.; Chang, H.W. A Particle Swarm Optimization-Based Approach with Local Search for Predicting Protein Folding. J. Comput. Biol. 2017, 24, 981\u2013994. [CrossRef] Symmetry 2024, 16, 1290 27 of 27 12. Bergh, F. A Cooperative approach to particle swarm optimization. IEEE Trans. Evol. Comput. 2004, 8, 225\u2013239. 13. Liang, J.J.; Qin, A.K.; Suganthan, P.N.; Baskar, S. Comprehensive learning particle swarm optimizer for global optimization of multimodal functions. IEEE Trans. Evol. Comput. 2006, 10, 281\u2013295. [CrossRef] 14. Huang, H.; Qin, H.; Hao, Z.F.; Lim, A. Example-based learning particle swarm optimization for continuous optimization. Inf. Sci. 2012, 182, 125\u2013138. [CrossRef] 15. Lynn, N.; Suganthan, P.N. Heterogeneous comprehensive learning particle swarm optimization with enhanced exploration and exploitation. Swarm Evol. Comput. 2015, 24, 11\u201324. [CrossRef] 16. Zhang, X.M.; Lin, Q.Y. Three-learning strategy particle swarm algorithm for global optimization problems. Inf. Sci. Int. J. 2022, 593, 289\u2013313. [CrossRef] 17. Xu,", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 45, "text": "182, 125\u2013138. [CrossRef] 15. Lynn, N.; Suganthan, P.N. Heterogeneous comprehensive learning particle swarm optimization with enhanced exploration and exploitation. Swarm Evol. Comput. 2015, 24, 11\u201324. [CrossRef] 16. Zhang, X.M.; Lin, Q.Y. Three-learning strategy particle swarm algorithm for global optimization problems. Inf. Sci. Int. J. 2022, 593, 289\u2013313. [CrossRef] 17. Xu, G.P.; Cui, Q.L.; Shi, X.H.; Ge, H.W.; Zhan, Z.H.; Lee, H.P.; Liang, Y.C.; Tai, R.; Wu, C.G. Particle swarm optimization based on dimensional learning strategy. Swarm Evol. Comput. 2019, 45, 33\u201351. [CrossRef] 18. Cai, L.; Hou, Y.; Zhao, Y.; Wang, J. Application research and improvement of particle swarm optimization algorithm. In Proceedings of the 2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS), Shenyang, China, 28\u201330 July 2020; pp. 238\u2013241. 19. Garg, H. A hybrid PSO-GA algorithm for constrained optimization problems. Appl. Math. Comput. 2016, 274, 292\u2013305. [CrossRef] 20. Uriarte, A.; Melin, P.; Valdez, F. An improved Particle Swarm Optimization algorithm applied to Benchmark Functions. In Proceedings of the 2016 IEEE 8th International Conference on Intelligent Systems (IS), Sofia, Bulgaria, 4\u20136 September 2016; pp. 128\u2013132. 21. Wang, X.H.; Li, J.J. Hybrid particle swarm optimization with simulated annealing. In Proceedings of the 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826), Shanghai, China, 26\u201329 August 2004; pp. 2402\u20132405. 22. Aydilek, I.B. A hybrid firefly and particle swarm optimization algorithm for computationally expensive numerical problems. Appl. Soft Comput. 2018, 66, 232\u2013249. [CrossRef] 23. Liang, B.X.; Zhao, Y.; Li, Y. A hybrid particle swarm optimization with crisscross learning strategy. Eng. Appl. Artif. Intell. 2021, 105, 104418. [CrossRef] 24. Zhang, X.W.; Liu, H.; Zhang, T.; Wang, Q.W.; Tu, L.P. Terminal crossover and steering-based particle swarm optimization algorithm with disturbance. Appl. Soft Comput. 2019, 85, 105841. [CrossRef] 25. Babak, Z.A. No-free-lunch-theorem: A page taken from the computational intelligence for water resources", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 46, "text": "Eng. Appl. Artif. Intell. 2021, 105, 104418. [CrossRef] 24. Zhang, X.W.; Liu, H.; Zhang, T.; Wang, Q.W.; Tu, L.P. Terminal crossover and steering-based particle swarm optimization algorithm with disturbance. Appl. Soft Comput. 2019, 85, 105841. [CrossRef] 25. Babak, Z.A. No-free-lunch-theorem: A page taken from the computational intelligence for water resources planning and management. Environ. Sci. Pollut. Res. Int. 2023, 30, 57212\u201357218. 26. Xu, L.; Zhu, S.; Wen, N. Deep reinforcement learning and its applications in medical imaging and radiation therapy: A survey. Phys. Med. Biol. 2022, 67, 22. [CrossRef] [PubMed] 27. Watkins, C.J.C.H. Learning from Delayed Rewards. Ph.D. Thesis, Cambridge University, Cambridge, UK, 1989. 28. Suganthan, P.N.; Hansen, N.; Liang, J.J.; Deb, K.; Tiwari, S. Problem Definitions and Evaluation Criteria for the CEC 2005 Special Session on Real-Parameter Optimization. In Natural Computing; Nanyang Technological University: Singapore, 2005; pp. 341\u2013357. 29. Liu, H.; Zhang, Y.; Tu, L.; Wang, Y. Human Behavior-Based Particle Swarm Optimization: Stability Analysis. In Proceedings of the 2018 37th Chinese Control Conference (CCC), Wuhan, China, 25\u201327 July 2018; pp. 3139\u20133144. 30. Liu, Y.J. A hierarchical simple particle swarm optimization with mean dimensional information. Appl. Soft Comput. 2019, 76, 712\u2013725. [CrossRef] 31. Arora, S.; Singh, S. Butterfly optimization algorithm: A novel approach for global optimization. Soft Comput. 2019, 23, 715\u2013734. [CrossRef] 32. Jain, M.; Maurya, S.; Rani, A.; Singh, V. Owl search algorithm: A novel nature-inspired heuristic paradigm for global optimization. J. Intell. Fuzzy Syst. 2018, 34, 1573\u20131582. [CrossRef] 33. Sahoo, S.K.; Saha, A.K.; Nama, S.; Masdari, M. An improved moth flame optimization algorithm based on modified dynamic opposite learning strategy. Artif. Intell. Rev. 2022, 56, 2811\u20132869. [CrossRef] 34. Wang, Y.J.; Yang, Y.P. Particle swarm optimization with preference order ranking for multi-objective optimization. Inf. Sci. 2009, 179, 1944\u20131959. [CrossRef] 35. Deng, W.; Zhao, H.M.; Yang, X.H.; Xiong, J.X.; Sun,", "paper_id": "symmetry-16-01290", "section": ""}
{"chunk_id": 47, "text": "improved moth flame optimization algorithm based on modified dynamic opposite learning strategy. Artif. Intell. Rev. 2022, 56, 2811\u20132869. [CrossRef] 34. Wang, Y.J.; Yang, Y.P. Particle swarm optimization with preference order ranking for multi-objective optimization. Inf. Sci. 2009, 179, 1944\u20131959. [CrossRef] 35. Deng, W.; Zhao, H.M.; Yang, X.H.; Xiong, J.X.; Sun, M.; Li, B. Study on an improved adaptive PSO algorithm for solving multi-objective gate assignment. Appl. Soft Comput. 2017, 59, 288\u2013302. [CrossRef] Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.", "paper_id": "symmetry-16-01290", "section": ""}
